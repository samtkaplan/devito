{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Devito Performance modes\n",
    "\n",
    "This tutorial illustrates the impact of several Devito optimization modes to the generated code.\n",
    "\n",
    "An Operator has two preset optimization levels: `noop` and `advanced`.  With `noop`, no performance optimizations are introduced by the compiler. With `advanced`, several flop-reducing and data locality optimizations are applied. Examples of flop-reducing optimizations are common sub-expressions elimination and factorization; examples of data locality optimizations are loop fusion and cache blocking. SIMD vectorization is also applied through compiler auto-vectorization.\n",
    "\n",
    "To choose the performance optimization level one should set the env variable DEVITO_OPT. By default it is set to the maximum level, `advanced`.\n",
    "\n",
    "e.g. `export DEVITO_OPT=advanced`\n",
    "\n",
    "alternaltively we can set the opt level at runtime using:\n",
    "\n",
    "`configuration['opt'] = advanced`\n",
    "\n",
    "or pass it directly to an `Operator` like:\n",
    "\n",
    "```\n",
    "eq = Eq(u.forward, u+2)\n",
    "op = Operator(eq, opt='advanced')\n",
    "```\n",
    "\n",
    "We will use an `Operator` with the `Equation` `eq = Eq(u.forward, u.dx.dx)` in order to .....\n",
    "\n",
    "Several optimizations will be incrementally applied starting from `noop` level and step by step going towards the `advanced` level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to print the difference between the generated code.\n",
    "def _unidiff_output(expected, actual):\n",
    "    \"\"\"\n",
    "    Helper function. Returns a string containing the unified diff of two multiline strings.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    expected=expected.splitlines(1)\n",
    "    actual=actual.splitlines(1)\n",
    "\n",
    "    diff=difflib.unified_diff(expected, actual)\n",
    "\n",
    "    return ''.join(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import clear_cache\n",
    "import numpy as np\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import Grid, TimeFunction, Eq, Operator, clear_cache\n",
    "from examples.cfd import plot_field, init_hat\n",
    "from devito import Eq, solve\n",
    "from devito import configuration\n",
    "\n",
    "# Initialise our problem parameters\n",
    "nx = 200\n",
    "ny = 200\n",
    "nz = 200\n",
    "\n",
    "grid = Grid(shape=(nx, ny, nz))\n",
    "u = TimeFunction(name='u', grid=grid, space_order=2)\n",
    "eq = Eq(u.forward, u.dx.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell illustrates the diff between an `Operator` that is not optimized at all, versus an `Operator` that is exploting OpenMP parallelism. We notice from the printed diff that the OpenMP code has:\n",
    "\n",
    " - a header `#include \"omp.h\"`\n",
    "as well as OpenMP directives:\n",
    " - `#pragma omp parallel num_threads(nthreads)`\n",
    " - `#pragma omp for collapse(...) schedule(dynamic,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "op_noop = Operator(eq, opt=('noop', {'openmp': False}))\n",
    "op_noop_openmp = Operator(eq, opt=('noop', {'openmp': True}))\n",
    "\n",
    "print(_unidiff_output(str(op_noop), str(op_noop_openmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showed the diff in the code by enabling OpenMP parallelism. Let's have a look at the code when we enable some data-locality optimizations. The next cell prints the generated code when enabling cache blocking in addition to OpenMP parallelism. Notice the new `bf0` function where we iterate through blocks, still using OpenMP parallelism. The size of the blocks is by default decided form the autotuner unless explicitly defined by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "op_blocking = Operator(eq, opt=('blocking', {'openmp': True}))\n",
    "print(op_blocking.ccode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better performance we avoid blocking the innermost loop. Vectorizing the innermost loop is performing better in the general case. However, if someone wants to, loop blocking the innermost loop can be set as shown in the next cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_blocking_inner = Operator(eq, opt=('blocking', {'blockinner': True, 'openmp': True}))\n",
    "# print(op_blocking_inner.ccode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add SIMD vectorization in the generated code. The next cell shows the addition of vectorization in the innermost `z loop` compared to basic loop blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "op_blocking_simd = Operator(eq, opt=('blocking', 'simd', {'openmp': True}))\n",
    "print(_unidiff_output(str(op_blocking), str(op_blocking_simd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However Devito *by default*, optimizes even more the generated code. This happens when Devito optimization level is set to `advanced` as previoulsly stated in the introductory first cell. Let's have a look at the diff between the `advanced` optimization level versus loop blocking and SIMD vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "op_advanced = Operator(eq, opt=('advanced', {'openmp': True}))\n",
    "print(_unidiff_output(str(op_blocking_simd), str(op_advanced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code diff in the cell above shows the differences between these two versions.\n",
    "First of all, we notice the addition of \n",
    "```\n",
    "+#include \"xmmintrin.h\"\n",
    "+#include \"pmmintrin.h\"\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```\n",
    "+  /* Flush denormal numbers to zero in hardware */\n",
    "+  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);\n",
    "+  _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n",
    "```\n",
    "Denormals are normally flushed when using SSE-based instruction sets, except when compiling shared objects.\n",
    "\n",
    "Then, at the PDE update we see that we get `float r0` out of the main update expression as it is a common expression that can we avoid computing again and again.\n",
    "\n",
    "This flop reduction after symbolic optimization results to an `Operator` that is computationally cheaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "configuration['log-level']='DEBUG'\n",
    "op_blocking_simd = Operator(eq, opt=('blocking', 'simd', {'openmp': True}))\n",
    "op_advanced = Operator(eq, opt=('advanced', {'openmp': True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 15-flop expression\n",
    "\n",
    "`u[t1][x + 2][y + 2][z + 2] = -(-u[t0][x + 2][y + 2][z + 2]/h_x + u[t0][x + 3][y + 2][z + 2]/h_x)/h_x + (-u[t0][x + 3][y + 2][z + 2]/h_x + u[t0][x + 4][y + 2][z + 2]/h_x)/h_x;`\n",
    "\n",
    "is now a 6-flop\n",
    "\n",
    "`r2[xs][ys][z] = (-u[t0][x + 3][y + 2][z + 2] + u[t0][x + 4][y + 2][z + 2])/h_x;`\n",
    "plus\n",
    "`u[t1][x + 2][y + 2][z + 2] = (-r2[xs][ys][z] + r2[xs + 1][ys][z])/h_x;`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest to always run with `opt='advanced'` (set by default) unless preferred otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Michael E. Wolf and Monica S. Lam. 1991. A data locality optimizing algorithm. In Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation (PLDI ’91). Association for Computing Machinery, New York, NY, USA, 30–44. DOI:https://doi.org/10.1145/113445.113449"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "459.85px",
    "left": "1723.22px",
    "right": "20px",
    "top": "120px",
    "width": "352.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
